{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os,sys\n",
    "import caffe\n",
    "import lmdb\n",
    "import caffe.proto.caffe_pb2\n",
    "from caffe.io import datum_to_array\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import cv2\n",
    "import tempfile\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from batch_norm import batch_norm_2D\n",
    "import myutils\n",
    "import PoseTools\n",
    "import localSetup\n",
    "import operator\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poseEvalNet(lin,locs,conf,trainPhase,dropout):\n",
    "\n",
    "    lin_sz = tf.Tensor.get_shape(lin).as_list()\n",
    "    lin_numel = reduce(operator.mul, lin_sz[1:], 1)\n",
    "    lin_re = tf.reshape(lin,[-1,lin_numel])\n",
    "    lin_re = tf.nn.dropout(lin_re,dropout)\n",
    "    with tf.variable_scope('lin_fc'):\n",
    "        weights = tf.get_variable(\"weights\", [lin_numel, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        lin_fc = tf.nn.relu(batch_norm_2D(tf.matmul(lin_re,weights)+biases,trainPhase))\n",
    "\n",
    "        \n",
    "    loc_sz = tf.Tensor.get_shape(locs).as_list()\n",
    "    loc_numel = reduce(operator.mul, loc_sz[1:], 1)\n",
    "    loc_re = tf.reshape(locs,[-1,loc_numel])\n",
    "    with tf.variable_scope('loc_fc'):\n",
    "        weights = tf.get_variable(\"weights\", [loc_numel, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        loc_fc = tf.nn.relu(batch_norm_2D(tf.matmul(loc_re,weights)+biases,trainPhase))\n",
    "        \n",
    "    joint_fc = tf.concat(1,[lin_fc,loc_fc])\n",
    "    \n",
    "    with tf.variable_scope('fc1'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt*2, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        joint_fc1 = tf.nn.relu(batch_norm_2D(tf.matmul(joint_fc,weights)+biases,trainPhase))\n",
    "\n",
    "    with tf.variable_scope('fc2'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        joint_fc2 = tf.nn.relu(batch_norm_2D(tf.matmul(joint_fc1,weights)+biases,trainPhase))\n",
    "        \n",
    "    with tf.variable_scope('out'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt, 2],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", 2,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        out = tf.matmul(joint_fc2,weights)+biases\n",
    "        \n",
    "    layer_dict = {'lin_fc':lin_fc,\n",
    "                  'loc_fc':loc_fc,\n",
    "                  'joint_fc1':joint_fc1,\n",
    "                  'joint_fc2':joint_fc2,\n",
    "                  'out':out\n",
    "                 }\n",
    "    return out,layer_dict\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createEvalPH(conf):\n",
    "    lin = tf.placeholder(tf.float32,[None,conf.n_classes,conf.nfcfilt])\n",
    "    locs = tf.placeholder(tf.float32,[None,conf.n_classes,2])\n",
    "    learning_rate_ph = tf.placeholder(tf.float32,shape=[])\n",
    "    y = tf.placeholder(tf.float32,[None,2])\n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')                 \n",
    "    dropout = tf.placeholder(tf.float32, shape=[])                 \n",
    "    phDict = {'lin':lin,'locs':locs,'learning_rate':learning_rate_ph,\n",
    "              'y':y,'phase_train':phase_train,'dropout':dropout}\n",
    "    return phDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFeedDict(phDict):\n",
    "    feed_dict = {phDict['lin']:[],\n",
    "                 phDict['locs']:[],\n",
    "                 phDict['y']:[],\n",
    "                 phDict['learning_rate']:1.,\n",
    "                 phDict['phase_train']:False,\n",
    "                 phDict['dropout']:1.\n",
    "                }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createEvalSaver(conf):\n",
    "    evalSaver = tf.train.Saver(var_list = PoseTools.getvars('poseEval'),max_to_keep=conf.maxckpt)\n",
    "    return evalSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadEval(sess,conf,iterNum):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.evaloutname)\n",
    "    ckptfilename = '%s-%d'%(outfilename,iterNum)\n",
    "    print('Loading base from %s'%(ckptfilename))\n",
    "    self.basesaver.restore(sess,ckptfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restoreEval(sess,conf,evalSaver,restore=True):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.evaloutname)\n",
    "    latest_ckpt = tf.train.get_checkpoint_state(conf.cachedir,\n",
    "                                        latest_filename = conf.evalckptname)\n",
    "    if not latest_ckpt or not restore:\n",
    "        startat = 0\n",
    "        sess.run(tf.initialize_variables(PoseTools.getvars('poseEval')))\n",
    "        print(\"Not loading eval variables. Initializing them\")\n",
    "        didRestore = False\n",
    "    else:\n",
    "        evalSaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "        matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "        startat = int(matchObj.group(1))+1\n",
    "        print(\"Loading eval variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "        didRestore = True\n",
    "        \n",
    "    return didRestore,startat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveEval(sess,step,evalSaver,conf):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.evaloutname)\n",
    "    evalSaver.save(sess,outfilename,global_step=step,\n",
    "               latest_filename = conf.evalckptname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genLabels(rlocs,locs,conf):\n",
    "    d2locs = np.sqrt(((rlocs-locs[...,np.newaxis])**2).sum(-2))\n",
    "    ll = np.arange(1,conf.n_classes+1)\n",
    "    labels = np.tile(ll[:,np.newaxis],[d2locs.shape[0],1,d2locs.shape[2]])\n",
    "    labels[d2locs>conf.poseEvalNegDist] = -1.\n",
    "    labels[d2locs<conf.poseEvalNegDist] = 1.\n",
    "    labels = np.concatenate([labels[:,np.newaxis],1-labels[:,np.newaxis]],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genRandomNegSamples(bout,l7out,locs,conf,nsamples=10):\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*conf.rescale*conf.pool_scale\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.zeros(locs.shape + (nsamples,))\n",
    "    rlocs[:,:,0,:] = np.random.randint(sz[1],size=locs.shape[0:2]+(nsamples,))\n",
    "    rlocs[:,:,1,:] = np.random.randint(sz[0],size=locs.shape[0:2]+(nsamples,))\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genGaussianPosSamples(bout,l7out,locs,conf,nsamples=10,maxlen = 4):\n",
    "    scale = conf.rescale*conf.pool_scale\n",
    "    sigma = float(maxlen)*0.5*scale\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*scale\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.round(np.random.normal(size=locs.shape+(15*nsamples,))*sigma)\n",
    "    # remove rlocs that are far away.\n",
    "    dlocs = np.all( np.sqrt( (rlocs**2).sum(2))< (maxlen*scale),1)\n",
    "    clocs = np.zeros(locs.shape+(nsamples,))\n",
    "    for ii in range(dlocs.shape[0]):\n",
    "        ndx = np.where(dlocs[ii,:])[0][:nsamples]\n",
    "        clocs[ii,:,:,:] = rlocs[ii,:,:,ndx].transpose([1,2,0])\n",
    "\n",
    "    rlocs = locs[...,np.newaxis] + clocs\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genGaussianNegSamples(bout,l7out,locs,conf,nsamples=10,minlen = 8):\n",
    "    scale = conf.rescale*conf.pool_scale\n",
    "    sigma = minlen*scale\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*scale\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.round(np.random.normal(size=locs.shape+(5*nsamples,))*sigma)\n",
    "    # remove rlocs that are small.\n",
    "    dlocs = np.sqrt( (rlocs**2).sum(2)).sum(1)\n",
    "    clocs = np.zeros(locs.shape+(nsamples,))\n",
    "    for ii in range(dlocs.shape[0]):\n",
    "        ndx = np.where(dlocs[ii,:]> (minlen*conf.n_classes*scale) )[0][:nsamples]\n",
    "        clocs[ii,:,:,:] = rlocs[ii,:,:,ndx].transpose([1,2,0])\n",
    "\n",
    "    rlocs = locs[...,np.newaxis] + clocs\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genMovedNegSamples(bout,l7out,locs,conf,nsamples=10,minlen=8):\n",
    "    # Add same x and y to locs\n",
    "    \n",
    "    minlen = float(minlen)*1.25\n",
    "    maxlen = 2*minlen\n",
    "    rlocs = np.zeros(locs.shape + (nsamples,))\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*conf.rescale*conf.pool_scale\n",
    "\n",
    "    for curi in range(locs.shape[0]):\n",
    "        rx = np.round(np.random.rand(nsamples)*(maxlen-minlen) + minlen)*\\\n",
    "            np.sign(np.random.rand(nsamples)-0.5)\n",
    "        ry = np.round(np.random.rand(nsamples)*(maxlen-minlen) + minlen)*\\\n",
    "            np.sign(np.random.rand(nsamples)-0.5)\n",
    "\n",
    "        rlocs[curi,:,0,:] = locs[curi,:,0,np.newaxis] + rx*conf.rescale*conf.pool_scale\n",
    "        rlocs[curi,:,1,:] = locs[curi,:,1,np.newaxis] + ry*conf.rescale*conf.pool_scale\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genOneMovedNegSamples(bout,l7out,locs,conf,nsamples=10,minlen=8):\n",
    "    # Add same x and y to locs\n",
    "    \n",
    "    minlen = 1.5*float(minlen)\n",
    "    maxlen = 2*minlen\n",
    "    \n",
    "    rlocs = np.tile(locs[...,np.newaxis],[1,1,1,nsamples])\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*conf.rescale*conf.pool_scale\n",
    "\n",
    "    for curi in range(locs.shape[0]):\n",
    "        for curs in range(nsamples):\n",
    "            rx = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                np.sign(np.random.rand()-0.5)\n",
    "            ry = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                np.sign(np.random.rand()-0.5)\n",
    "            rand_point = np.random.randint(conf.n_classes)\n",
    "            \n",
    "            rlocs[curi,rand_point,0,curs] = locs[curi,rand_point,0] + rx*conf.rescale*conf.pool_scale\n",
    "            rlocs[curi,rand_point,1,curs] = locs[curi,rand_point,1] + ry*conf.rescale*conf.pool_scale\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genNegSamples(bout,l7out,locs,conf,nsamples=10):\n",
    "    minlen = 5\n",
    "    rlocs = np.concatenate([\n",
    "#                           genRandomNegSamples(bout,l7out,locs,conf,nsamples),\n",
    "                          genGaussianNegSamples(bout,l7out,locs,conf,nsamples,minlen),\n",
    "                          genMovedNegSamples(bout,l7out,locs,conf,nsamples,minlen), \n",
    "                          genOneMovedNegSamples(bout,l7out,locs,conf,nsamples,minlen)], \n",
    "                          axis=3)\n",
    "#     rlabels = genLabels(rlocs,locs,conf)\n",
    "    return rlocs#,rlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(l7out,inlocs,conf):\n",
    "    locs = np.round(inlocs/(conf.rescale*conf.pool_scale))\n",
    "    dd = np.zeros(locs.shape[0:2]+l7out.shape[-1:]+locs.shape[-1:])\n",
    "    for curi in range(locs.shape[0]):\n",
    "        for pp in range(locs.shape[1]):\n",
    "            for s in range(locs.shape[3]):\n",
    "                dd[curi,pp,:,s] = l7out[curi,int(locs[curi,pp,1,s]),int(locs[curi,pp,0,s]),:]\n",
    "    return dd        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareOpt(baseNet,dbtype,feed_dict,sess,conf,phDict,distort):\n",
    "    nsamples = 10\n",
    "    npos = 2\n",
    "    baseNet.updateFeedDict(dbtype,distort)\n",
    "    locs = baseNet.locs\n",
    "    l7 = baseNet.baseLayers['conv7']\n",
    "    [bout,l7out] = sess.run([baseNet.basePred,l7],feed_dict=baseNet.feed_dict)\n",
    "    neglocs = genNegSamples(bout,l7out,locs,conf,nsamples=nsamples)\n",
    "#     replocs = np.tile(locs[...,np.newaxis],npos*nsamples)\n",
    "    replocs = genGaussianPosSamples(bout,l7out,locs,conf,nsamples*npos,maxlen=4)\n",
    "    alllocs = np.concatenate([neglocs,replocs],axis=-1)\n",
    "    alllocs = alllocs.transpose([0,3,1,2])\n",
    "    alllocs = alllocs.reshape((-1,)+alllocs.shape[2:])\n",
    "    retlocs = copy.deepcopy(alllocs)\n",
    "    alllocs_m = alllocs.mean(1)\n",
    "    alllocs = alllocs-alllocs_m[:,np.newaxis,:]\n",
    "    \n",
    "#    poslabels = np.ones(replocs.shape[0,1,3])\n",
    "#    alllabels = np.concatenate([neglabels,poslabels],axis=-2)\n",
    "#    alllabels = alllabels.transpose([0,2,1,3])\n",
    "#    alllabels = alllabels.reshape((-1,)+alllabels.shape[-1])\n",
    "\n",
    "    negdd = genData(l7out,neglocs,conf)\n",
    "    posdd = genData(l7out,replocs,conf)\n",
    "    alldd = np.concatenate([negdd,posdd],axis=-1)\n",
    "    alldd = alldd.transpose([0,3,1,2])\n",
    "    alldd = np.reshape(alldd,[-1,alldd.shape[-2],alldd.shape[-1]])\n",
    "\n",
    "#     y = alllabels\n",
    "    y = np.zeros([l7out.shape[0],neglocs.shape[-1]+replocs.shape[-1],2])\n",
    "    y[:,:-nsamples*npos,0] = 1. \n",
    "    y[:,-nsamples*npos:,1] = 1.\n",
    "    y = np.reshape(y,[-1,y.shape[-1]])\n",
    "        \n",
    "#     excount = step*conf.batch_size\n",
    "#     cur_lr = learning_rate * \\\n",
    "#             conf.gamma**math.floor(excount/conf.step_size)\n",
    "#     feed_dict[phDict['learning_rate']] = cur_lr\n",
    "    feed_dict[phDict['y']] = y\n",
    "    feed_dict[phDict['lin']] = alldd\n",
    "    feed_dict[phDict['locs']] = alllocs\n",
    "    return retlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(conf,restore=True):\n",
    "    \n",
    "    phDict = createEvalPH(conf)\n",
    "    feed_dict = createFeedDict(phDict)\n",
    "    feed_dict[phDict['phase_train']] = True\n",
    "    feed_dict[phDict['dropout']] = 0.5\n",
    "    with tf.variable_scope('poseEval'):\n",
    "        out,layer_dict = poseEvalNet(phDict['lin'],phDict['locs'],\n",
    "                                     conf,phDict['phase_train'],\n",
    "                                     phDict['dropout'])\n",
    "        \n",
    "    evalSaver = createEvalSaver(conf)\n",
    "    y = phDict['y']\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out, y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-5).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(out,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    baseNet = PoseTools.createNetwork(conf,1)\n",
    "    baseNet.openDBs()\n",
    "    \n",
    "    with baseNet.env.begin() as txn,baseNet.valenv.begin() as valtxn,tf.Session() as sess:\n",
    "\n",
    "        baseNet.createCursors()\n",
    "        baseNet.restoreBase(sess,True)\n",
    "        didRestore,startat = restoreEval(sess,conf,evalSaver,restore)\n",
    "        baseNet.initializeRemainingVars(sess)\n",
    "        for step in range(startat,conf.eval_training_iters+1):\n",
    "            prepareOpt(baseNet,baseNet.DBType.Train,feed_dict,sess,conf,\n",
    "                       phDict,distort=True)\n",
    "#             baseNet.feed_dict[baseNet.ph['keep_prob']] = 0.5\n",
    "            feed_dict[phDict['phase_train']] = True\n",
    "            feed_dict[phDict['dropout']] = 0.5\n",
    "            sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "            if step % 25 == 0:\n",
    "                prepareOpt(baseNet,baseNet.DBType.Train,feed_dict,\n",
    "                           sess,conf,phDict,distort=False)\n",
    "#                 baseNet.feed_dict[baseNet.ph['keep_prob']] = 1\n",
    "                feed_dict[phDict['phase_train']] = False\n",
    "                feed_dict[phDict['dropout']] = 1\n",
    "                train_cross_ent = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                test_cross_ent = 0\n",
    "                test_acc = 0\n",
    "                pos_acc = 0\n",
    "                pred_acc_pos = 0\n",
    "                pred_acc_pred = 0\n",
    "                \n",
    "                #generate blocks for different neg types\n",
    "                nsamples = 10\n",
    "                ntypes = 3\n",
    "                npos = 2\n",
    "                blk = np.arange(nsamples)\n",
    "                inter = np.arange(0,conf.batch_size*nsamples*(ntypes+npos),nsamples*(ntypes+npos))\n",
    "                n_inters = blk + inter[:,np.newaxis]\n",
    "                n_inters = n_inters.flatten()\n",
    "                nacc = np.zeros(ntypes)\n",
    "                nclose = 0 \n",
    "                in_locs = np.array([])\n",
    "                nrep = 40\n",
    "                for rep in range(nrep):\n",
    "                    prepareOpt(baseNet,baseNet.DBType.Val,feed_dict,sess,conf,\n",
    "                               phDict,distort=False)\n",
    "                    test_cross_ent += sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                    test_acc += sess.run(accuracy, feed_dict = feed_dict)\n",
    "                    tout = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "                    labels = feed_dict[phDict['y']]\n",
    "                    pos_acc += float(np.count_nonzero(tout[labels[:,1]>0.5]))/nsamples\n",
    "                    for nt in range(ntypes):\n",
    "                        nacc[nt] += float(np.count_nonzero(tout[n_inters+nt*nsamples]))/nsamples\n",
    "                    \n",
    "                    tdd = feed_dict[phDict['lin']]\n",
    "                    tlocs = feed_dict[phDict['locs']]\n",
    "                    ty = feed_dict[phDict['y']]\n",
    "                    # \n",
    "                    l7 = baseNet.baseLayers['conv7']\n",
    "                    curpred = sess.run([baseNet.basePred,l7], feed_dict=baseNet.feed_dict)\n",
    "                    baseLocs = PoseTools.getBasePredLocs(curpred[0],conf)\n",
    "\n",
    "                    neglocs = baseLocs[:,:,:,np.newaxis]\n",
    "                    locs = np.array(baseNet.locs)[...,np.newaxis]\n",
    "                    d2locs = np.sqrt( np.sum((neglocs-locs)**2,axis=(1,2,3)))\n",
    "                    alllocs = np.concatenate([neglocs,locs],axis=3)\n",
    "                    alldd = genData(curpred[1],alllocs,conf)\n",
    "                    alllocs = alllocs.transpose([0,3,1,2])\n",
    "                    alllocs = alllocs.reshape((-1,)+alllocs.shape[2:])\n",
    "                    alllocs_m = alllocs.mean(1)\n",
    "                    alllocs = alllocs-alllocs_m[:,np.newaxis,:]\n",
    "\n",
    "                    alldd = alldd.transpose([0,3,1,2])\n",
    "                    alldd = np.reshape(alldd,[-1,alldd.shape[-2],alldd.shape[-1]])\n",
    "\n",
    "                    y = np.zeros([curpred[0].shape[0],alllocs.shape[-1],2])\n",
    "                    y[d2locs>=25,:-1,0] = 1. \n",
    "                    y[d2locs<25,:-1,1] = 1. \n",
    "                    y[:,-1,1] = 1.\n",
    "                    y = np.reshape(y,[-1,y.shape[-1]])\n",
    "\n",
    "                    feed_dict[phDict['y']] = y\n",
    "                    feed_dict[phDict['lin']] = alldd\n",
    "                    feed_dict[phDict['locs']] = alllocs\n",
    "\n",
    "                    corrpred = sess.run(correct_prediction,feed_dict=feed_dict)\n",
    "                    pred_acc_pos += np.count_nonzero(corrpred[1::2])\n",
    "                    pred_acc_pred += np.count_nonzero(corrpred[0::2])\n",
    "                    nclose += np.count_nonzero(d2locs<25)\n",
    "                    er_locs = ~corrpred[0::2]\n",
    "                    in_locs = np.append(in_locs,d2locs[er_locs])\n",
    "                print \"Iter:{:d}, train:{:.4f} test:{:.4f} acc:{:.2f} posacc:{:.2f}\".format(step,\n",
    "                                                 train_cross_ent,test_cross_ent/nrep,\n",
    "                                                 test_acc/nrep,pos_acc/nrep/conf.batch_size/npos)\n",
    "                print \"Neg:{}\".format(nacc/nrep/conf.batch_size)\n",
    "                print \"Pred Acc Pos:{},Pred Acc Pred:{},numclose:{}\".format(float(pred_acc_pos)/nrep/conf.batch_size,\n",
    "                                                                            float(pred_acc_pred)/nrep/conf.batch_size,\n",
    "                                                                            float(nclose)/nrep/conf.batch_size)\n",
    "                print 'Distance of incorrect predictions:{}'.format(in_locs)\n",
    "                \n",
    "            if step % 100 == 0:\n",
    "                saveEval(sess,step,evalSaver,conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
